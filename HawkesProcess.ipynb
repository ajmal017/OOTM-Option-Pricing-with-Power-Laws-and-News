{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# Hawkes Processes for earnings\n",
    "This notebook investigates how earnings reports change the parameters of the Hawkes processes fitting the motion of different Technology stocks. \n",
    "\n",
    "Implementation 1 shows a single-variable Hawkes process with a power law kernel, which works but is not applicable to the dataset.\n",
    "\n",
    "Implementation 2 shows a bivariate Hawkes process (+ and - movements) with an exponential kernel. The code is vectorized and runs fast. It is applicable to the data as it considers the size of the movements (marks) when determining how the event changes the process' intensity.\n",
    "\n",
    "Later, I will add a second kernel explaining changes in the point process due to preplanned corperate news releases, such as earnings.\n",
    "\n",
    "I will use this model to value near-term OOTM options around corperate events via simulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timezone, datetime, timedelta\n",
    "import requests\n",
    "import sys\n",
    "from scipy.optimize import minimize, SR1, Bounds\n",
    "import finnhub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPuWspVFpn6p"
   },
   "source": [
    "## Quick look at datasource (FinnHub API)\n",
    "\n",
    "Showing what data is available for the stocks. Skip forward for Hawkes Processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQSCP3M_yzK2"
   },
   "outputs": [],
   "source": [
    "# my finhub API token\n",
    "token = 'bsoarofrh5re01t4f9a0'\n",
    "finnhub_client = finnhub.Client(api_key=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([1, 2, 3, 4])\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(finnhub_client.company_basic_financials('AAPL', 'margin'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "oBIWOnID1Yv9",
    "outputId": "a60d49bc-4fb3-40f9-d2f7-5e38af356874"
   },
   "outputs": [],
   "source": [
    "d = finnhub_client.company_executive('AAPL')\n",
    "df = pd.DataFrame(d['executive'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "U8Clzqfb1x2D",
    "outputId": "67d57b12-ea13-4490-e869-307f32fd65bd"
   },
   "outputs": [],
   "source": [
    "print(finnhub_client.company_peers('AAPL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfQdXyx48mmY"
   },
   "outputs": [],
   "source": [
    "d = finnhub_client.financials_reported(symbol='AAPL')\n",
    "df = pd.DataFrame(d['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aNZY6mJRAzJO"
   },
   "outputs": [],
   "source": [
    "finnhub_client.company_profile(symbol='AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3BmYWH6JycE"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "d = finnhub_client.etfs_holdings('SPY')\n",
    "df = pd.DataFrame(d['holdings'])\n",
    "profiles = []\n",
    "for symbol in df['symbol']:\n",
    "  profiles.append(finnhub_client.company_profile(symbol=symbol))\n",
    "  time.sleep(1)\n",
    "  print(\"added:\",symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYZNtbn9SxKD"
   },
   "outputs": [],
   "source": [
    "df['gsector'] = [profiles[i]['gsector'] for i in range(len(profiles))]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "1RY69i3MTtSo",
    "outputId": "a0796619-7008-49ab-ec80-df2b16db9c59"
   },
   "outputs": [],
   "source": [
    "df['profiles'] = profiles\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SiT2Nnk9t8eI"
   },
   "outputs": [],
   "source": [
    "# get data for S&P companies\n",
    "d = finnhub_client.etfs_holdings('SPY')\n",
    "df = pd.DataFrame(d['holdings'])\n",
    "df2 = pd.DataFrame()\n",
    "for i in range(len(profiles)):\n",
    "  df2 = df2.append(pd.DataFrame(list([profiles[i].values()]), columns=list(profiles[i].keys())))\n",
    "df2['symbol'] = df2['ticker']\n",
    "df2 = df2.drop('ticker', axis=1)\n",
    "df3 = df.merge(df2, on='symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVDwpRblvlrH"
   },
   "outputs": [],
   "source": [
    "#df3.to_csv('./SP500_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdPs_KRyALTA"
   },
   "source": [
    "## Hawkes Processes\n",
    "\n",
    "In the following section, I fit constraints on a Hawkes process using a quasi-Newton optimization.\n",
    "\n",
    "To confirm that the determined parameters fit the dataset, I will use parametric bootstrapping. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzK26jbWu8Yj"
   },
   "source": [
    "I approximate a power law as the sum of exponential functions with power-law weights, to use a recurrence relation that reduces the log-likelihood calculation from $O(N^2)$ to $O(N)$.\n",
    "\n",
    "$\\phi_{PL} (\\tau | n, \\epsilon, \\tau _0) = \\frac{n}{Z} \\left( \\Sigma _{i=0}^{M-1} \\left( \\frac{1}{\\xi _i} \\right) ^{1 + \\epsilon} e^{-\\tau / \\xi_i} - S e ^{- \\tau / \\xi_{-1}} \\right)$\n",
    "\n",
    "for $\\xi_i = \\tau_0 m^i$ where $-1 \\le i \\lt M$\n",
    "\n",
    "Parameter Z makes sure $\\int_0^\\infty \\phi_{PL} (\\tau) d\\tau = n$ and $S$ makes sure that $\\phi_{PL} (0) = 0$, as market participants can only react to events after more than 0 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O86MnQICoIGN"
   },
   "outputs": [],
   "source": [
    "resol = 5\n",
    "start = datetime(2020, 1, 1).replace(tzinfo=timezone.utc).timestamp()\n",
    "end = datetime(2020, 7, 1).replace(tzinfo=timezone.utc).timestamp()\n",
    "\n",
    "def get_stock_data(ticker, start = start, end = end, resolution = resol):\n",
    "  \"\"\" for stock symbol, get data\n",
    "  @param ticker (str): ticker symbol\n",
    "  @param start: start date as unix timestamp or datetime\n",
    "  @param end: end date as unix timestamp or datetime\n",
    "  @param resolution (int): time resolution of data\n",
    "  @returns: Pandas DataFrame of stock data\n",
    "  \"\"\"\n",
    "  # convert to unix time stamp\n",
    "  if start is datetime:\n",
    "    start = start.replace(tzinfo=timezone.utc).timestamp()\n",
    "  if end is datetime:\n",
    "    end = end.replace(tzinfo=timezone.utc).timestamp()\n",
    "  \n",
    "  # Stock candles\n",
    "  res = finnhub_client.stock_candles(ticker, resolution, start, end)\n",
    "  try: return pd.DataFrame(res)\n",
    "  except: return pd.DataFrame()\n",
    "\n",
    "def get_df(df, resolution = resol):\n",
    "  \"\"\" Cleans data from API into form interpretable for Hawkes Process\n",
    "  @param df: output from get_stock_data() function\n",
    "  @returns: pd DataFrame ready for analysis\n",
    "  \"\"\"\n",
    "  df = df.drop(columns=['c','h','l'])\n",
    "  df['diff'] = np.log(df['o']/df['o'].shift(1))\n",
    "  df['date'] = df['t'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "  df['date'] = df['date'].apply(lambda x: x - timedelta(seconds = np.random.uniform(0,resolution*60)))\n",
    "  df['dt'] = df['date'] - df['date'].shift(1)\n",
    "  df['dt'] = df['dt'].apply(lambda x: x.total_seconds() / 100)\n",
    "\n",
    "  # check that data import was mostly valid\n",
    "  len_before = df.shape[0]\n",
    "  df = df.drop(df.loc[df['s'] != 'ok'].index, axis = 0)\n",
    "  if ( ((len_before - df.shape[0])/len_before) > .1 ):\n",
    "    raise ValueError(\"Over 10% of dataframe returned NA\")\n",
    "  df = df.drop(columns = ['s','t','o'])\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  # remove changes between days (will model those differently)\n",
    "  df = df.drop(df.loc[df['dt'] > 100].index, axis=0).iloc[1:]\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  # get only top 20% of movements\n",
    "  q1 = df['diff'].quantile(q=.1)\n",
    "  q2 = df['diff'].quantile(q=.9)\n",
    "\n",
    "  # create mask for top 20% movement\n",
    "  df['event'] = 0\n",
    "  df.loc[(df['diff'] < q1) | (df['diff'] > q2), 'event'] = 1\n",
    "\n",
    "  # get time elapsed\n",
    "  df['time_elapsed'] = 0\n",
    "  prev = 0\n",
    "  for i in range(1,df.shape[0]):\n",
    "    prev += df.loc[i, 'dt']\n",
    "    df.loc[i, 'time_elapsed'] = prev\n",
    "  \n",
    "  # mark positive movements\n",
    "  df['pos'] = 0\n",
    "  df.loc[df['diff'] > 0, 'pos'] = 1\n",
    "  df['diff'] = np.abs(df['diff'])\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VRe6zxWGr_wF"
   },
   "outputs": [],
   "source": [
    "sp500 = pd.read_csv('./company_data/SP500_info.csv')\n",
    "tech = sp500.loc[sp500['finnhubIndustry'] == 'Technology']\n",
    "tech = tech.reset_index(drop=True)\n",
    "tech = tech.drop(columns=['Unnamed: 0'])\n",
    "tech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQVXYOLp5njT"
   },
   "outputs": [],
   "source": [
    "# get earnings dates for all tech stocks in S&P\n",
    "earnings = []\n",
    "for i in range(len(tech)):\n",
    "  ticker = tech['symbol'].iloc[i]\n",
    "  earnings.append(pd.DataFrame(finnhub_client.earnings_calendar(symbol=ticker)['earningsCalendar']))\n",
    "earnings = pd.concat(earnings)\n",
    "earnings = earnings.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "DxmVU1zVfJ1v",
    "outputId": "4600808f-ab03-4bc0-e2b2-f0b1b044eec2"
   },
   "outputs": [],
   "source": [
    "earnings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m19qXtAFfOq1"
   },
   "outputs": [],
   "source": [
    "#earnings.to_csv('./earnings_data/tech_earnings.csv')\n",
    "earnings = pd.read_csv('./earnings_data/tech_earnings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EV_LRpXMzSy9"
   },
   "outputs": [],
   "source": [
    "# calculate ten days prior for string\n",
    "from pandas.tseries.offsets import BDay # for business days\n",
    "\n",
    "def ten_prior(d):\n",
    "  return (d - BDay(10)).to_pydatetime()\n",
    "\n",
    "def this_friday(d):\n",
    "  ## CHECK THAT THIS INCLUDES FRIDAY TRADING IN DATA\n",
    "  return d + timedelta( (5-d.weekday()) % 7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggzVdcXKy7i6"
   },
   "outputs": [],
   "source": [
    "earnings['date'] = earnings['date'].apply(lambda s: datetime.strptime(s, '%Y-%m-%d'))\n",
    "earnings['ten_prior'] = earnings['date'].apply(lambda d: ten_prior(d))\n",
    "earnings['this_friday'] = earnings['date'].apply(lambda d: this_friday(d))\n",
    "earnings['performance'] = (earnings['epsActual'] - earnings['epsEstimate']) / earnings['epsEstimate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "aFDsxlsjHV09",
    "outputId": "5ddaf651-aaff-43aa-929e-9c280bbf30c3"
   },
   "outputs": [],
   "source": [
    "earnings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "5mtG1VYH9hSO",
    "outputId": "2e2507ba-8d37-4f46-cb68-d70b496523b1"
   },
   "outputs": [],
   "source": [
    "len(earnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "colab_type": "code",
    "id": "y5JQIqCZtFoq",
    "outputId": "837e4187-ce4d-465d-db3d-41b2498e08c5"
   },
   "outputs": [],
   "source": [
    "# get financial data for the dataframe\n",
    "for i in range(len(earnings)):\n",
    "  symbol = earnings.loc[i, 'symbol']\n",
    "  start = int(datetime.timestamp(earnings.loc[i, 'ten_prior']))\n",
    "  end = int(datetime.timestamp(earnings.loc[i, 'this_friday']))\n",
    "  date = earnings.loc[i, 'date'].date() # date not timestamp\n",
    "\n",
    "  try: df = get_df(get_stock_data(symbol, start = start, end = end))\n",
    "  except: continue\n",
    "  path = './earnings_data/five_minute_data/' + symbol + '_' + str(date) + '.csv'\n",
    "  df.to_csv(path)\n",
    "\n",
    "  if (i % 5) == 0:\n",
    "    print(i / len(earnings), \"percent complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "0on4ZIdE_Tz-",
    "outputId": "570cbd11-e85c-40b2-ac37-24f3de114dd3"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from power_laws import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "colab_type": "code",
    "id": "B4__YIxwEHB7",
    "outputId": "7560e38d-c528-40d9-c2a0-8d763075e7b6"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "files = listdir('./earnings_data/five_minute_data')\n",
    "f_names = [name for name in files if re.search('AAPL', name)]\n",
    "path = './earnings_data/five_minute_data/' + f_names[0]\n",
    "df = pd.read_csv(path)\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sort by move size and calculate cdf\n",
    "\n",
    "df_pos = df.loc[df['pos'] == 1]\n",
    "pos_sorted = df_pos.sort_values('diff')\n",
    "pos_sorted['prob'] = 1 - np.arange(pos_sorted.shape[0]) / pos_sorted.shape[0]\n",
    "pos_sorted = pos_sorted.reset_index(drop=True)\n",
    "\n",
    "df_neg = df.loc[df['pos'] == 0]\n",
    "neg_sorted = df_neg.sort_values('diff')\n",
    "neg_sorted['prob'] = 1 - np.arange(neg_sorted.shape[0]) / neg_sorted.shape[0]\n",
    "neg_sorted = neg_sorted.reset_index(drop=True)\n",
    "\n",
    "# pos_sorted.tail()\n",
    "# \n",
    "#calc_emperical(pos_sorted)\n",
    "show_power_law(neg_sorted=neg_sorted)\n",
    "#neg_pos_sortedsorted.loc[:,'diff'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "colab_type": "code",
    "id": "Mw1OG_q8qS9N",
    "outputId": "d4471a14-f29f-413a-90b8-f0b625a1d94b"
   },
   "outputs": [],
   "source": [
    "import tkinter\n",
    "show_power_law(pos_sorted=pos_sorted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "Mt1xiOOkrkrZ",
    "outputId": "ba2382dc-2766-4006-f90d-3fa636611b3d"
   },
   "outputs": [],
   "source": [
    "# verify that it's a power law\n",
    "p = calc_emperical(pos_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "QXcpsPTBsZmY",
    "outputId": "9dd98774-dfe7-4915-e5f6-07b316ecd36a"
   },
   "outputs": [],
   "source": [
    "# not a power law... trying negative\n",
    "p = calc_emperical(neg_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KC-PT81itbZS"
   },
   "source": [
    "While the graph may look like a power law, it is not the best fit for stock price movements at the 5 minute time frame. This makes sense as stocks have limits on how far they can move in 5 minutes. An exponential or lognormal distribution might fit the data better. I will try to fit a gamma distribution using MLE.\n",
    "\n",
    "That being said, the influence of an event on the intensity of a Hawkes process might still decay at a power law rate, which is what I will investigate next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "miARcamexqje",
    "outputId": "834e47e4-cd8a-49f9-ae87-b545646fa3b4"
   },
   "outputs": [],
   "source": [
    "X = df.loc[df['pos']==0,'diff'].to_numpy()\n",
    "X = X - np.min(X)\n",
    "l = expo_MLE(X)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "RxwvGLeQ2Lqp",
    "outputId": "78e050d6-6ae8-49d3-e5cf-6e60290a7a10"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTRxLrFyrI-r"
   },
   "source": [
    "I will define an outlier for the Hawkes process as a point beyond the Karmatta point where movements after follow a power law distribution. Using the power law $\\alpha$ obtained from MLE methods, I will optimize the Hawkes process to predict these movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1deK6Fv6smA"
   },
   "source": [
    "Hawke's MLE approach from https://arxiv.org/abs/1405.6047"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFrMZM6fnwG1"
   },
   "source": [
    "## Implementation 1)\n",
    "single variable hawkes process modeling timings of large movements. Hawkes process does not apply to the data used right now, but implemenation works.\n",
    "\n",
    "Implementation 2 is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXu3oMzbtnBT"
   },
   "outputs": [],
   "source": [
    "# max log likelihood using power law kernel without news kernel\n",
    "# from itertools import *\n",
    "\n",
    "# def pl_likelihood(params, df):\n",
    "#   \"\"\" the power law kernel (sum of 15 exponentials) \n",
    "#   @params (floats): mu, n, p, t0 (see formulas)\n",
    "#   @returns (float): negative log likelihood for those parameters\n",
    "#   \"\"\"\n",
    "#   mu, n, p, t0 = params\n",
    "#   print(\"testing mu = %s, n = %s, p = %s, t0 = %s\" % (mu,n,p,t0))\n",
    "#   X = df # work around rn\n",
    "#   M = 15\n",
    "#   m = 5\n",
    "#   T = X['time_elapsed'].iloc[-1]\n",
    "#   N = X.shape[0] # number of events\n",
    "#   a_neg1 = t0*m**(-1)\n",
    "  \n",
    "#   A_cache = dict([(0,t0)])\n",
    "#   def A(k):\n",
    "#     if k in A_cache:\n",
    "#       return A_cache[k]\n",
    "#     A_cache[k] = t0*m**k\n",
    "#     return A_cache[k]\n",
    "\n",
    "#   # calculate S so PL(0) = 0\n",
    "#   # make the first index the first event...\n",
    "#   S = np.sum([(A(k))**(-p) for k in range(M-1)])\n",
    "#   #print(\"S:\",S)\n",
    "\n",
    "#   # calculate integral of phi power law 0 to inf to get Z\n",
    "#   phi_pl = 0\n",
    "\n",
    "#   for i in range(0,N):\n",
    "#     ti = X['time_elapsed'].iloc[i]\n",
    "#     term1 = np.sum([(A(k))**(-p)*np.exp(-ti/(A(k))) for k in range(M-1)])\n",
    "#     phi_pl = phi_pl + n*(term1 - S*np.exp(-ti/a_neg1))\n",
    "#   Z = phi_pl / n\n",
    "#   #print(\"Z =\",Z)\n",
    "\n",
    "#   # def phi(t):\n",
    "#   #   \"\"\" power law kernel for t\n",
    "#   #   \"\"\"\n",
    "#   #   term1 = np.sum([(A(k))**(-p)*np.exp(-t/(A(k))) for k in range(M-1)])\n",
    "#   #   return n/Z*(term1 - S*np.exp(-t/A(-1)))\n",
    "\n",
    "#   # v_phi = np.vectorize(phi)\n",
    "#   # calculate mu\n",
    "\n",
    "#   # AHHHH neither of these methods are efficient... need to calculate mu another way\n",
    "\n",
    "#   # time_elapsed = X['time_elapsed'].to_numpy()\n",
    "#   # differences = [a-b for (a,b) in combinations(time_elapsed,2)]\n",
    "\n",
    "#   # big_lambda = 0\n",
    "#   # times = np.array([0])\n",
    "#   # for i in range(1,N):\n",
    "#   #   ti = X['time_elapsed'].iloc[i]\n",
    "#   #   local_times = np.repeat(ti, i) - time_elapsed[0:i]\n",
    "#   #   times = np.concatenate((times,local_times))\n",
    "\n",
    "#   # ncombos = N*(N-1)/2\n",
    "#   # big_lambda = np.sum(v_phi(differences))/ncombos\n",
    "\n",
    "#   # big_lambda = 0\n",
    "#   # for i in range(1,N):\n",
    "#   #   print(i)\n",
    "#   #   # calculate the t values for list\n",
    "#   #   ti = X['time_elapsed'].iloc[i]\n",
    "#   #   times = np.full((1,i),ti) - X['time_elapsed'].iloc[0:i].to_numpy()\n",
    "#   #   big_lambda += n/Z*(np.sum(v_phi(times)))\n",
    "#   #mu = -big_lambda/(1-(1/(1-n)))\n",
    "\n",
    "#   likelihood = -mu*T\n",
    "\n",
    "#   # memoization speeds it up a lot\n",
    "#   cache = dict([(1,0)])\n",
    "#   def RK(ind,k):\n",
    "#     if ind in cache:\n",
    "#       return cache[ind]\n",
    "#     ti = X['time_elapsed'].iloc[ind]\n",
    "#     ti_minus1 = X['time_elapsed'].iloc[ind-1]\n",
    "#     val = np.exp(-(ti - ti_minus1)/(t0*m**k))*(1+RK(ind-1,k))\n",
    "#     cache[ind] = val\n",
    "#     return val\n",
    "  \n",
    "#   cache2 = dict([(1,0)])\n",
    "#   def R1(ind):\n",
    "#     if ind in cache2:\n",
    "#       return cache2[ind]\n",
    "#     ti = X['time_elapsed'].iloc[ind]\n",
    "#     ti_minus1 = X['time_elapsed'].iloc[ind-1]\n",
    "#     val = np.exp(-(ti - ti_minus1)/(t0*m**-1))*(1+R1(ind-1))\n",
    "#     cache2[ind] = val\n",
    "#     return val\n",
    "\n",
    "#   for i in range(1,N):\n",
    "#     ti = X['time_elapsed'].iloc[i]\n",
    "#     term1 = np.sum([(A(k))**(1-p)*(1-np.exp(-(T-ti)/(A(k)))) for k in range(0,M)])\n",
    "#     term2 = S/(a_neg1)*(1 - np.exp(-(T - ti)/a_neg1))\n",
    "#     logsum1 = np.sum([(A(k))**(-p)*RK(i,k) for k in range(0,M)])\n",
    "#     #print(R1(i))\n",
    "#     logterm1 = np.log(mu + n/Z*(logsum1 - S*R1(i)))\n",
    "#     likelihood = likelihood - n/Z*(term1 - term2) + logterm1\n",
    "#     #print(likelihood)\n",
    "\n",
    "#   print(\"Negative Likelihood:\",-likelihood) # minimize negative likelihood!!\n",
    "#   return -likelihood # want to maximize likelihood with minimization function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "colab_type": "code",
    "id": "u6YbS5P8241f",
    "outputId": "4102b54c-6055-41bc-c68e-ba0637816532"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import SR1, Bounds\n",
    "\n",
    "#set initial value guesses for mu, n, p, t0. (.7,.9),(1.2,1.4),(1.2,3)\n",
    "rand = np.random.uniform\n",
    "params = np.array([rand(0,.5),rand(),rand(1,2),rand(1,2)])\n",
    "# set linear constraint\n",
    "bounds = Bounds([0, 0, 1.0, .6],[.5, 1.1, 2, 5])\n",
    "res = minimize(pl_likelihood, params, method=\"L-BFGS-B\",bounds = bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "lqhIlexjQUbA",
    "outputId": "dca21d48-477d-4d4f-8603-a35217acbf56"
   },
   "outputs": [],
   "source": [
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUOoIurIlKWP"
   },
   "source": [
    "## Implementation 2\n",
    "\n",
    "The below function creates a bivariate Hawkes process (+ and - movements) efficiently using vectorization.\n",
    "\n",
    "I need to fix the starting parameters on the optimization, but the code runs. \n",
    "\n",
    "Built based off of https://people.math.ethz.ch/~embrecht/ftp/Hawkes_PE_TL_LL.pdf and the author's doctoral thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGbLgHE9qXMM"
   },
   "source": [
    "The Log Likelihood function for a multivariate (dimensions $= d$) and single value marks Hawkes Process:\n",
    "\n",
    "$\\Lambda_j (t) := \\int_{T_*}^{t} {\\lambda_j (s) ds}$\n",
    "\n",
    "$\\log L = \\Sigma _{j=1}^d \\int_{DxR} \\log \\lambda_j (t) N_j (dt \\times dx) + \\Sigma _{j=1}^d \\int _{D\\times R} \\log f_j (x) N_j (dt \\times dx) - \\Sigma _{j=1}^d \\Lambda_j (T^*)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEId260Mvswe"
   },
   "outputs": [],
   "source": [
    "# now fit model that works\n",
    "# decay function w(t)\n",
    "def w(t,dirac):\n",
    "  \"\"\" expo() pdf \"\"\"\n",
    "  if t < 0:\n",
    "    return 0\n",
    "  return dirac*np.exp(-dirac*t)\n",
    "v_w = np.vectorize(w)\n",
    "\n",
    "def w_hat(t,dirac):\n",
    "  \"\"\" cdf of expo() \"\"\"\n",
    "  if t < 0:\n",
    "    return 0\n",
    "  return (1-np.exp(-dirac*t))\n",
    "v_w_hat = np.vectorize(w_hat)\n",
    "\n",
    "def g(x, l, b):\n",
    "  \"\"\" normalized impact function where alpha_k =1\n",
    "  @param x (float): mark of point\n",
    "  @param l (float): lambda represents rate of expo() dist for mark\n",
    "  @param b (float): beta represents parameter for impact of mark\n",
    "  \"\"\"\n",
    "  return (l**2 / (l**2 + b*l) * (1 + b*x))\n",
    "v_g = np.vectorize(g)\n",
    "\n",
    "pos_cache = dict([(1,0),(0,0),(-1,0)])\n",
    "neg_cache = dict([(1,0),(0,0),(-1,0)])\n",
    "def R(length, times, pos, dirac):\n",
    "  \"\"\" breaks up sum with recursion trick \n",
    "  @param times (np.array): the arrival times\n",
    "  @param pos (int): 1 if positive 0 if negative (helps with caching)\n",
    "  \"\"\"\n",
    "  i = length - 1 # calculate index\n",
    "  if pos:\n",
    "    if i in pos_cache:\n",
    "      return pos_cache[i]\n",
    "    t = times[i] - times[i-1]\n",
    "    pos_cache[i] = np.exp(-dirac*t)*(1+R((i-1),times,pos,dirac))\n",
    "    return pos_cache[i]\n",
    "  if i in neg_cache:\n",
    "    return neg_cache[i]\n",
    "  t = times[i] - times[i-1]\n",
    "  neg_cache[i] = np.exp(-dirac*t)*(1+R((i-1),times,pos,dirac))\n",
    "  return neg_cache[i]\n",
    "\n",
    "def multi_likelihood(params, l1, l2, X):\n",
    "  \"\"\" \n",
    "  @param n1,n2 (floats): underlying intensity for process\n",
    "  @param o11,o12,o21,o22 (floats): branching coefficients in matrix Q\n",
    "  @param b1,b2 (ints): control impact function - influence of mark on intensity\n",
    "  @param dirac (float): parameter for decay function w()\n",
    "  @param l1,l2 (ints): lambda defines expo() pdf of marks\n",
    "  @param X (np.array): df.loc[:,['time_elapsed','pos','diff']].to_numpy()\n",
    "  @returns (float): negative log likelihood for those parameters\n",
    "  \"\"\"\n",
    "  d = 2\n",
    "  T = X[-1,0] # time in sequence\n",
    "  N = X.shape[0] # number of events\n",
    "\n",
    "  n1,n2,o11,o12,o21,o22,b1,b2,dirac = params\n",
    "  n = np.array([n1,n2])\n",
    "  o = np.array([[o11,o12],[o21,o22]])\n",
    "  b = np.array([b1,b2])\n",
    "  l = np.array([l1,l2])\n",
    "\n",
    "  # Lambda_j\n",
    "  likelihood = -np.sum([n[j] * (T) for j in range(d)])\n",
    "  for j in range(d):\n",
    "    for k in range(d):\n",
    "      indices = np.where(X[:-1,1] == j)[0]  # indices where it's positive or negative\n",
    "      current_ts = T - np.take(X[:-1,0], indices) # get differences of all times\n",
    "      decay = v_w_hat(current_ts,dirac)\n",
    "      current_marks = np.take(X[:-1,2], indices)\n",
    "      impact = v_g(current_marks, l[k], b[k])\n",
    "      likelihood -= o[j,k] * np.sum(decay*impact)\n",
    "\n",
    "  # log (lambda_j)\n",
    "  for j in range(d):\n",
    "    for i in range(1,N):\n",
    "      t = X[i,0]\n",
    "      term1 = 0\n",
    "      for k in range(d):\n",
    "        gx = g(X[i,2], l[k], b[k])\n",
    "        indices = np.where(X[:i,1]==j)[0]\n",
    "        times = np.take(X[:i,0],indices)\n",
    "        term1 += o[j,k] * gx * R(len(times), times, j, dirac)\n",
    "      likelihood += np.log(n[j] + term1)\n",
    "\n",
    "  # log (f_j). (CHECK THIS WITH ALEX Fj() DEFINITION)\n",
    "  for j in range(d):\n",
    "    indices = np.where(X[:i,1]==j)[0]\n",
    "    marks = np.take(X[:i,2],indices)\n",
    "    likelihood += np.sum(np.log(v_w(marks,dirac)))\n",
    "\n",
    "  print(\"Negative Likelihood:\",-likelihood) # minimize negative likelihood!!\n",
    "  return -likelihood # want to maximize likelihood with minimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZGpw_Ztw3TU"
   },
   "outputs": [],
   "source": [
    "# get MLE for expo\n",
    "def expo_MLE(marks):\n",
    "  \"\"\" gets lambda of expo() distribution for marks\n",
    "  @param marks (np.array): all the poins\n",
    "  @returns (float): lambda for expo() distributin\n",
    "  \"\"\"\n",
    "  return len(marks) / np.sum(marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDqPfokNyhhy"
   },
   "outputs": [],
   "source": [
    "# get expo() dist fits for marks\n",
    "X = df.loc[df['pos']==0,'diff'].to_numpy()\n",
    "X = X - np.min(X)\n",
    "l1 = np.round(expo_MLE(X))\n",
    "\n",
    "X = df.loc[df['pos']==1,'diff'].to_numpy()\n",
    "X = X - np.min(X)\n",
    "l2 = np.round(expo_MLE(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "2gYMO14za71-",
    "outputId": "6771c904-8acf-450a-eed5-3b3b01396692"
   },
   "outputs": [],
   "source": [
    "#set initial guesses for 9 parameters\n",
    "# n1,n2,o11,o12,o21,o22,b1,b2,dirac\n",
    "X = df.loc[:,['time_elapsed','pos','diff']].to_numpy()\n",
    "rand = np.random.uniform\n",
    "params = np.random.uniform(.1,.9,size=9)\n",
    "# set linear constraint\n",
    "bounds = Bounds([0]*9,[np.inf]*9)\n",
    "#res = minimize(multi_likelihood, params, method=\"L-BFGS-B\",bounds = bounds)\n",
    "res = minimize(multi_likelihood, params, args = (l1,l2,X), method='trust-constr',  jac=\"2-point\", \n",
    "               hess=SR1(), options = {'maxiter':15}, bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "s7p884ul0jwW",
    "outputId": "eb3debf9-4e61-4f23-8361-a400610baffb"
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "from numpy.linalg import eig\n",
    "n1,n2,o11,o12,o21,o22,b1,b2,dirac = res.x\n",
    "Q = np.array([[o11,o12],[o21,o22]])\n",
    "print(\"Q:\")\n",
    "print(Q)\n",
    "print(\"Max Eigenvalue =\",np.max(eig(Q)[0]))\n",
    "print(\"n1 = %s n2 = %s\" % (n1,n2))\n",
    "print(\"b1 = %s b2 = %s dirac = %s\" % (b1,b2,dirac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQb3zjXbhXL_"
   },
   "source": [
    "## Hawkes Process Simulation\n",
    "To test whether this works, I will simualate a bivariate Hawkes process and try to reproduce the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "colab_type": "code",
    "id": "h8LWRdJEYYUA",
    "outputId": "fd2607d8-e7ef-4a60-ac5a-d174b723550f"
   },
   "outputs": [],
   "source": [
    "from tick.hawkes import SimuHawkesExpKernels\n",
    "from tick.plot import plot_point_process\n",
    "\n",
    "n_nodes = 2  # dimension of the Hawkes process\n",
    "adjacency = .1 * np.ones((n_nodes, n_nodes))\n",
    "adjacency[0, 1] = 0\n",
    "DECAY = 3\n",
    "decays = DECAY * np.ones((n_nodes, n_nodes))\n",
    "baseline = 0.5 * np.ones(n_nodes)\n",
    "hawkes = SimuHawkesExpKernels(adjacency=adjacency, decays=decays,\n",
    "                              baseline=baseline, verbose=False, seed=2398)\n",
    "\n",
    "run_time = 1000\n",
    "hawkes.end_time = run_time\n",
    "dt = 0.01\n",
    "hawkes.track_intensity(dt)\n",
    "hawkes.simulate()\n",
    "\n",
    "fig, ax = plt.subplots(n_nodes, 1, figsize=(16, 8), sharex=True, sharey=True)\n",
    "plot_point_process(hawkes, n_points=50000, t_min=10, max_jumps=80, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "PKuLxyCLZmOO",
    "outputId": "8f8dae0e-5f71-433c-b89f-77dbfe1bf2b3"
   },
   "outputs": [],
   "source": [
    "# want to create array of (ti, di, xi):\n",
    "# simulate iid marks \n",
    "lambda1 = 30\n",
    "lambda2 = 40\n",
    "neg_marks = np.random.exponential(scale=lambda1,size=len(hawkes.timestamps[0]))\n",
    "l1 = expo_MLE(neg_marks)\n",
    "print(1/lambda1,\"vs estimated:\", l1)\n",
    "pos_marks = np.random.exponential(scale=lambda2,size=len(hawkes.timestamps[1]))\n",
    "l2 = expo_MLE(pos_marks)\n",
    "print(1/lambda2,\"vs estimated:\",l2)\n",
    "neg_times = np.column_stack((hawkes.timestamps[0], np.zeros_like(hawkes.timestamps[0]),neg_marks))\n",
    "pos_times = np.column_stack((hawkes.timestamps[1], np.ones_like(hawkes.timestamps[1]),pos_marks))\n",
    "times = np.concatenate((neg_times,pos_times))\n",
    "times = times[times[:,0].argsort()]\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eqKE4Hiia7f_",
    "outputId": "7b0f24fd-5abd-4bae-b797-d931e81efebb"
   },
   "outputs": [],
   "source": [
    "X = times\n",
    "# n1,n2,o11,o12,o21,o22,b1,b2,dirac\n",
    "params = np.random.uniform(.1,.9,size=9)\n",
    "bounds = Bounds([0]*9,[np.inf]*9)\n",
    "\n",
    "res = minimize(multi_likelihood, params, args = (l1,l2,X), \n",
    "               method='trust-constr', jac=\"2-point\", \n",
    "               hess=SR1(), options = {'maxiter':30}, bounds=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5X8Lx65PKnjX"
   },
   "source": [
    "As the code below shows, the method isn't working right now, but I should fix it soon...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "P1ARWiyrqORQ",
    "outputId": "bb15829c-1f6a-4364-d85a-ce88623ae402"
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "from numpy.linalg import eig\n",
    "\n",
    "n1,n2,o11,o12,o21,o22,b1,b2,dirac = res.x\n",
    "Q = np.array([[o11,o12],[o21,o22]])\n",
    "\n",
    "# correct results:\n",
    "print(\"Adjacency Real\", adjacency)\n",
    "print(\"Adjancency Found\",Q)\n",
    "print(\"Baseline Real\",baseline)\n",
    "print(\"Baseline Found\",n1,n2)\n",
    "print(\"Spectral Radius Real:\",hawkes.spectral_radius())\n",
    "print(\"Spectral Radius Found:\", np.max(eig(Q)[0]))\n",
    "print(\"b1 = %s b2 = %s dirac = %s\" % (b1,b2,dirac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "5GM5UGawDiKn",
    "outputId": "75256e41-7f85-457d-b857-7fc2118a284c"
   },
   "outputs": [],
   "source": [
    "hawkes.spectral_radius()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "U3rF_PLszscS",
    "outputId": "4747ef7f-df31-4630-d5ff-1b09af6316e2"
   },
   "outputs": [],
   "source": [
    "from tick.plot import plot_hawkes_kernels\n",
    "from tick.hawkes import SimuHawkesSumExpKernels, SimuHawkesMulti, \\\n",
    "    HawkesSumExpKern\n",
    "\n",
    "end_time = 1000\n",
    "n_realizations = 10\n",
    "\n",
    "decays = [.5, 2., 6.]\n",
    "baseline = [0.12, 0.07]\n",
    "adjacency = [[[0, .1, .4], [.2, 0., .2]],\n",
    "             [[0, 0, 0], [.6, .3, 0]]]\n",
    "\n",
    "hawkes_exp_kernels = SimuHawkesSumExpKernels(\n",
    "    adjacency=adjacency, decays=decays, baseline=baseline,\n",
    "    end_time=end_time, verbose=False, seed=1039)\n",
    "\n",
    "multi = SimuHawkesMulti(hawkes_exp_kernels, n_simulations=n_realizations)\n",
    "\n",
    "multi.end_time = [(i + 1) / 10 * end_time for i in range(n_realizations)]\n",
    "multi.simulate()\n",
    "\n",
    "learner = HawkesSumExpKern(decays, penalty='elasticnet',\n",
    "                           elastic_net_ratio=0.8)\n",
    "learner.fit(multi.timestamps)\n",
    "\n",
    "plot_hawkes_kernels(learner, hawkes=hawkes_exp_kernels, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDr4pkxb43E-"
   },
   "source": [
    "My implementation is much slower than the tick.Hawkes, so for now on, I'm going to use the tick.Hawkes to fit the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "DOe7OxxFJUy1",
    "outputId": "87bd17bc-a6bc-4dfd-acf1-e0ab4fa31c93"
   },
   "outputs": [],
   "source": [
    "# fit the times to a bivariate hawkes w/ exp decay kernel\n",
    "from tick.plot import plot_hawkes_kernels\n",
    "from tick.hawkes import SimuHawkesSumExpKernels, SimuHawkesMulti, \\\n",
    "    HawkesSumExpKern, SimuHawkesExpKernels, HawkesExpKern\n",
    "\n",
    "end_time = 1000\n",
    "n_realizations = 10\n",
    "\n",
    "decays = np.array([[.1,.1],[.1,.1]])\n",
    "baseline = np.array([.05,.02])\n",
    "adjacency = np.array([[.8,0],[.1,.75]])\n",
    "\n",
    "hawkes_exp_kernels = SimuHawkesExpKernels(adjacency=adjacency, \n",
    "                                          decays=decays, baseline=baseline,\n",
    "                                          end_time=end_time,verbose=False,\n",
    "                                          seed=1008)\n",
    "# simulate bivariate hawkes\n",
    "multi = SimuHawkesMulti(hawkes_exp_kernels, n_simulations=n_realizations)\n",
    "\n",
    "multi.end_time = [(i + 1) / 10 * end_time for i in range(n_realizations)]\n",
    "multi.simulate()\n",
    "\n",
    "learner = HawkesExpKern(decays, penalty='elasticnet',\n",
    "                           elastic_net_ratio=0.8)\n",
    "learner.fit(multi.timestamps)\n",
    "\n",
    "#plot_hawkes_kernels(learner, hawkes=hawkes_exp_kernels, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "p7LWnBloOFTe",
    "outputId": "267d5e40-d7b5-43d7-c10f-5b0515cc090c"
   },
   "outputs": [],
   "source": [
    "learner.adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "coEO99hOU7ag",
    "outputId": "c48667fc-6715-4a57-fc00-55ba08f63872"
   },
   "outputs": [],
   "source": [
    "learner.baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "cUqz0y0fVPfY",
    "outputId": "a44b1a1f-1f14-424d-d086-6ff8ec048ec6"
   },
   "outputs": [],
   "source": [
    "learner.decays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "qMU4fKw2WSn_",
    "outputId": "02b2fdc5-676b-4289-e08e-99f57bec0aca"
   },
   "outputs": [],
   "source": [
    "# fit to my data\n",
    "X = df.loc[:,['time_elapsed','pos','diff']].to_numpy()\n",
    "time1 = X[np.where(X[:,1]==0),0][0]\n",
    "time2 = X[np.where(X[:,1]==1),0][0]\n",
    "timestamps = list((time1, time2))\n",
    "scores = []\n",
    "for i, d in enumerate(np.linspace(.01,.5,num=10)):\n",
    "  decays = np.array([[d,d],[d,d]])\n",
    "  learner = HawkesExpKern(decays, penalty='elasticnet',\n",
    "                           elastic_net_ratio=0.8)\n",
    "  learner.fit(timestamps)\n",
    "  #scores.append(learner.score(events=timestamps))\n",
    "  #print(learner.adjacency)\n",
    "  #print(learner.baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "Wv72pIysaWcp",
    "outputId": "c647bd48-51d7-4fd6-e1eb-79c6ea63eb85"
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GX2Msg3HcPW7"
   },
   "outputs": [],
   "source": [
    "from tick.plot import plot_point_process\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8), sharex=True, sharey=True)\n",
    "plot_point_process(timestamps, n_points=50000, t_min=10, max_jumps=30, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkLhCTJnS5nT"
   },
   "source": [
    "\n",
    "\n",
    "# Fit to my data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzDYSou2Hcy3"
   },
   "source": [
    "To build the model, I want to select similar stocks with 5 days before earnings and 5 days after (check what timeframe makes sense). I want to model each trading day as a stochastic process with different parameters that change based on the stock type, earnings report and the difference between previous day close and current open.\n",
    "\n",
    "The jumpy diffusion Hawkes process model seems to make the most sense. However, I will add an event kernel with parameters that change based on the earnings report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fojlIoKIgNN"
   },
   "outputs": [],
   "source": [
    "# import the earnings data\n",
    "from os import listdir\n",
    "import re\n",
    "files = listdir('./earnings_data/five_minute_data')\n",
    "f_names = [name for name in files if re.search('AAPL', name)]\n",
    "\n",
    "# determine whether earnings released before or after trading\n",
    "earnings = pd.read_csv('./earnings_data/tech_earnings.csv')\n",
    "earnings = earnings.drop(columns = 'Unnamed: 0')\n",
    "earnings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CuWReung9rwJ"
   },
   "outputs": [],
   "source": [
    "# for one stock, read in earnings file and estimate parameters\n",
    "f = f_names[1]\n",
    "path = './earnings_data/five_minute_data/' + f\n",
    "X = pd.read_csv(path)\n",
    "X = X.drop(columns = 'Unnamed: 0')\n",
    "X['date'] = X['date'].apply(lambda d: datetime.strptime(d, '%Y-%m-%d %H:%M:%S.%f'))\n",
    "\n",
    "date_string = f.split(\"_\")[1].split(\".\")[0]\n",
    "date_dt = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "ticker = f.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hour = earnings.loc[(earnings['date'] == date_string) & (earnings['symbol']==ticker),'hour'].values[0]\n",
    "first_day_released = (date_dt + timedelta(days=1)) if (hour == 'amc') else date_dt # during market hours would be different\n",
    "\n",
    "# get slices for before and after earnings\n",
    "X_before = X.loc[X['date'] < first_day_released]\n",
    "X_after = X.loc[X['date'] >= first_day_released]\n",
    "\n",
    "X_before.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "7etoOn_uFdzC",
    "outputId": "2eb6e2cd-e00f-4568-ca7a-a4029af241d0"
   },
   "outputs": [],
   "source": [
    "# group by day\n",
    "days = np.sort(list(set(X_before['date'].apply(lambda x: x.date()))))\n",
    "for day in days:\n",
    "  print(day)\n",
    "\n",
    "days_after = np.sort(list(set(X_after['date'].apply(lambda x: x.date()))))\n",
    "for day in days_after:\n",
    "  print(day)\n",
    "\n",
    "all_days = np.concatenate([days, days_after])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeBgvMwLd1EU"
   },
   "outputs": [],
   "source": [
    "days = list(map(lambda d: datetime(d.year,d.month,d.day), all_days))\n",
    "# days = list(days)\n",
    "# days = v_to_datetime(days)\n",
    "subsets = []\n",
    "for i in range(len(days)):\n",
    "  subsets.append(X.loc[(X['date'] > days[i]) & (X['date'] < days [i] + timedelta(days=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "2VqpivnOEg1h",
    "outputId": "5d76854b-3049-4150-8e8b-eb07cadb1d60"
   },
   "outputs": [],
   "source": [
    "for i in range(len(subsets)):\n",
    "  print(subsets[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5nxuVYPwHuc"
   },
   "source": [
    "Because I'm trying to predict the arrival frequency of the process after the release of earnings, I will define the threshold for an arrival based on the ten days before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "-yOKejgXwecG",
    "outputId": "d2bb1224-ec5f-4b19-926b-b9169d07b256"
   },
   "outputs": [],
   "source": [
    "# get only top 20% of movements\n",
    "q1 = X_before['diff'].quantile(q=.1)\n",
    "q2 = X_before['diff'].quantile(q=.9)\n",
    "\n",
    "# create mask for top 20% movement\n",
    "X_before.loc[:,'event'] = 0\n",
    "X_before.loc[(X_before['diff'] < q1) | (X_before['diff'] > q2), 'event'] = 1\n",
    "\n",
    "# overlay that mask on the post earnings data\n",
    "X_after.loc[:,'event'] = 0\n",
    "X_after.loc[(X_after['diff'] < q1) | (X_after['diff'] > q2), 'event'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ii-8uzMgsts3"
   },
   "outputs": [],
   "source": [
    "first5 = X_before.loc[(X_before['date'] < days[len(days) // 2])]\n",
    "second5 = X_before.loc[(X_before['date'] >= days[len(days) // 2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "lDewc91KtPmJ",
    "outputId": "29fe9ba5-1c88-4ef7-eec0-fd002d3e2f52"
   },
   "outputs": [],
   "source": [
    "second5['time_elapsed'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6XO1uBtwaIb"
   },
   "outputs": [],
   "source": [
    "from tick.plot import plot_hawkes_kernels\n",
    "from tick.hawkes import SimuHawkesSumExpKernels, SimuHawkesMulti, \\\n",
    "    HawkesSumExpKern, SimuHawkesExpKernels, HawkesExpKern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "s6Y5EvkstT2a",
    "outputId": "9a80cd45-8643-43e9-f6bc-79ff00d70fd8"
   },
   "outputs": [],
   "source": [
    "# run hawkes process fit on first five, second five and earnings\n",
    "X = X_after.loc[:,['time_elapsed','pos','diff']].to_numpy()\n",
    "time1 = X[np.where(X[:,1]==0),0][0]\n",
    "time2 = X[np.where(X[:,1]==1),0][0]\n",
    "timestamps = list((time1, time2))\n",
    "scores = []\n",
    "for i, d in enumerate(np.linspace(.01,.05,num=10)):\n",
    "  decays = np.array([[d,d],[d,d]])\n",
    "  learner = HawkesExpKern(decays, penalty='elasticnet',\n",
    "                           elastic_net_ratio=0.8)\n",
    "  learner.fit(timestamps)\n",
    "  scores.append(learner.score(events=timestamps))\n",
    "  print(learner.adjacency)\n",
    "  #print(learner.baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "-CjhaRD1yMZj",
    "outputId": "8418cbff-27e0-4d8f-eec0-20b69d9e8c68"
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7k05tUY1Te_"
   },
   "outputs": [],
   "source": [
    "# function from stack overflow\n",
    "def axvlines(xs, ax=None, **plot_kwargs):\n",
    "    \"\"\"\n",
    "    Draw vertical lines on plot\n",
    "    :param xs: A scalar, list, or 1D array of horizontal offsets\n",
    "    :param ax: The axis (or none to use gca)\n",
    "    :param plot_kwargs: Keyword arguments to be passed to plot\n",
    "    :return: The plot object corresponding to the lines.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xs = np.array((xs, ) if np.isscalar(xs) else xs, copy=False)\n",
    "    lims = ax.get_ylim()\n",
    "    x_points = np.repeat(xs[:, None], repeats=3, axis=1).flatten()\n",
    "    y_points = np.repeat(np.array(lims + (np.nan, ))[None, :], repeats=len(xs), axis=0).flatten()\n",
    "    plot = ax.plot(x_points, y_points, scaley = False, **plot_kwargs)\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "Spm69Gw91VJ-",
    "outputId": "1ad259a3-ed72-42d9-fdc1-9fe95371d679"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "axvlines(time2[:100]) # positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "tiRKWwMZjZCM",
    "outputId": "498d6cbd-3f63-45ed-9aa0-ad7e656e660b"
   },
   "outputs": [],
   "source": [
    "axvlines(time1[:100]) # negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETeW7kdo2DFk"
   },
   "outputs": [],
   "source": [
    "# for each day in times, print graph\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMo+fbaxQBmQQOH4H3sId2W",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "HawkesProcess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}